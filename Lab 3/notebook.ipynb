{
  "cells": [
    {
      "metadata": {
        "state": "normal"
      },
      "cell_type": "markdown",
      "source": "# Lab 03: N-grams\n\n## Overview\n\nAn *n-gram* -- in the context of parsing natural languages such as English -- is a sequence of *n* consecutive *tokens* (which we might define as characters separated by whitespace) from some passage of text. Based on the following passage:\n\n> I really really like cake.\n\nWe have the following 2-grams:\n\n    [('I', 'really'), ('really', 'really'), ('really', 'like'), ('like', 'cake.')]\n\nAnd the following 3-grams:\n\n    [('I', 'really', 'really'),\n     ('really', 'really', 'like'),\n     ('really', 'like', 'cake.')]\n\n(I omit a 1-gram listing because it would merely be a list of all tokens in the original text.)\n\nAmong other things, n-grams are useful for describing the vocabulary of and statistical correlation between tokens in a sample body of text (e.g., as taken from a book). We can use an n-gram model to determine the likelihood of finding\na particular sequence of words after another. This information, in turn, can be used to generate passages of text that statistically mimic the sample.\n\nWe can convert the above 3-gram list into the following lookup structure (i.e., a dictionary mapping strings to lists of 2-tuples), where the first token of each n-gram maps to all sequences that follow it in the text:\n\n    {'I': [('really', 'really')],\n     'really': [('really', 'like'), ('like', 'cake.')]}\n\nWe can now generate passages of text using the following method:\n\n1. Select a random key and use it as the start token of the passage. It will also serve as the current token for the next step.\n2. Select a random tuple from the list associated with the current token and append the sequence to the passage. The last token of the selected sequence will be the new current token.\n3. If the current token is a key in the dictionary then simply repeat step 2, otherwise select another random key from the dictionary as the current token and append it to the passage before repeating step 2.\n\nE.g., we might start by selecting `'I'` in step (1), which gives us `('really', 'really')` as our only choice in (2). The second `'really'` in that tuple is the new current token (which is a valid key), which takes us back to (2) and gives us a choice between two tuples. If we choose `('like', 'cake.')`, then we have `'cake.'` as our new current token --- it is not a key in the map, however, so we'd have to choose a new random key if we wanted to generate a longer passage. Either way, the passage we've generated thus far is `'I really really like cake.'` (which also happens to be the original passage).\n\nHere's a lengthier passage that could be generated from the 3-gram dictionary above -- note that for clarity I've added `*`'s every time a new random key is selected (i.e., when the previous token isn't a key in the dictionary):\n\n> \\* really like cake. \\* I really really really like \\* really like cake. \\* I really really really like \\* really\n\nThis gets more interesting when we build n-gram dictionaries from lengthier bodies of text. For instance, the following text was generated (with a little programmed embellishment for prettier capitalization and punctuation) from a 3-gram dictionary extracted from Romeo's famous balcony monologue:\n\n> Lamp her eyes were there they in their spheres till they in her eyes in all the fairest stars in all the heaven having some business do wear it is my love! O it is envious her cheek would through the heaven having some business do entreat her eyes were there they in their spheres till they in her eyes to.\n\nFor reference, here is the dictionary entry for the token `'her'` used to generate the above:\n\n    'her': [('maid', 'art'),\n            ('maid', 'since'),\n            ('vestal', 'livery'),\n            ('eyes', 'to'),\n            ('eyes', 'were'),\n            ('head?', 'The'),\n            ('cheek', 'would'),\n            ('eyes', 'in'),\n            ('cheek', 'upon'),\n            ('hand!', 'O')],\n\nIf you haven't already guessed it, your assignment is to implement a function that constructs an n-gram dictionary from a list of strings (tokens), and another that returns a passage of text generated from a given n-gram dictionary."
    },
    {
      "metadata": {
        "state": "normal"
      },
      "cell_type": "markdown",
      "source": "## Implementation Details\n\nBefore you start working on the aforementioned functions, it's important to consider how we'll be parsing passages for tokens.\n\nHere's the body of Romeo's balcony soliloquy:"
    },
    {
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "romeo",
        "nbgrader": {
          "grade": false,
          "grade_id": "romeo",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "state": "read_only",
        "trusted": true
      },
      "cell_type": "code",
      "source": "ROMEO_SOLILOQUY = \"\"\"\n        But, soft! what light through yonder window breaks?\n        It is the east, and Juliet is the sun.\n        Arise, fair sun, and kill the envious moon,\n        who is already sick and pale with grief, \n        That thou her maid art far more fair than she:\n        be not her maid, since she is envious;\n        her vestal livery is but sick and green\n        and none but fools do wear it; cast it off.\n        It is my lady, O, it is my love! \n        O, that she knew she were!\n        She speaks yet she says nothing: what of that?\n        Her eye discourses; I will answer it.\n        I am too bold, 'tis not to me she speaks:\n        two of the fairest stars in all the heaven, \n        having some business, do entreat her eyes\n        to twinkle in their spheres till they return.\n        What if her eyes were there, they in her head?\n        The brightness of her cheek would shame those stars,\n        as daylight doth a lamp; her eyes in heaven \n        would through the airy region stream so bright\n        that birds would sing and think it were not night.\n        See, how she leans her cheek upon her hand!\n        O, that I were a glove upon that hand,\n        that I might touch that cheek!\"\"\"",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "state": "normal"
      },
      "cell_type": "markdown",
      "source": "Using the string's built-in `split` method --- previously mentioned in class --- along with `lower`, we can derive from the passage a list of tokens."
    },
    {
      "metadata": {
        "state": "normal",
        "trusted": true
      },
      "cell_type": "code",
      "source": "toks = [t.lower() for t in ROMEO_SOLILOQUY.split()]\n\ntoks[:8]",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 2,
          "data": {
            "text/plain": "['but,', 'soft!', 'what', 'light', 'through', 'yonder', 'window', 'breaks?']"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "state": "normal"
      },
      "cell_type": "markdown",
      "source": "We could do more interesting things (such as separating out punctuation), but we'll keep our parser simple. For the sake of consistency, we'll rely on this fairly straighttforward approach to parsing. Onwards!\n\n### `compute_ngrams`\n\nYour first task is to write `compute_ngrams`, which will take a list of tokens, a value `n` indicating the n-gram length (e.g., 3 for 3-grams), and return an n-gram dictionary. The keys in the returned dictionary should all be strings, whose values will be lists of one or more tuples. Note that even in the case of `n`=2 (which would be the minimum value) the dictionary should map strings to lists of 1-tuples (i.e., instead of to lists of individual tokens)."
    },
    {
      "metadata": {
        "deletable": false,
        "id": "compute_ngrams",
        "nbgrader": {
          "grade": false,
          "grade_id": "compute_ngrams",
          "locked": false,
          "schema_version": 1,
          "solution": true
        },
        "starter_code": "def compute_ngrams(toks, n=2):\n    \"\"\"Returns an n-gram dictionary based on the provided list of tokens.\"\"\"\n    ",
        "state": "graded",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def compute_ngrams(toks, n=2):\n    \"\"\"Returns an n-gram dictionary based on the provided list of tokens.\"\"\"\n    #set and dic is unique\n    #len of tok - n + 1\n    dic = {toks[i]:[] for i in range(0, len(toks) - n + 1)} # makes a dictionary of all possible keys\n    for i in range(0, len(toks) - n + 1): #assigns tuples to keys\n        test = []\n        for j in range(1, n):\n            test.append(toks[i+j])\n        dic[toks[i]].append(tuple(test))\n    return(dic)",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "state": "normal"
      },
      "cell_type": "markdown",
      "source": "And now for some simple tests:"
    },
    {
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "test_compute_ngrams",
        "nbgrader": {
          "grade": true,
          "grade_id": "test_compute_ngrams",
          "locked": true,
          "points": 5,
          "schema_version": 1,
          "solution": false
        },
        "state": "read_only",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# (5 points)\n\nfrom unittest import TestCase\ntc = TestCase()\n\nsimple_toks = [t.lower() for t in 'I really really like cake.'.split()]\n\ncompute_ngrams(simple_toks)\ntc.assertEqual(compute_ngrams(simple_toks), \n               {'i': [('really',)], 'like': [('cake.',)], 'really': [('really',), ('like',)]})\ntc.assertEqual(compute_ngrams(simple_toks, n=3), \n               {'i': [('really', 'really')],\n                'really': [('really', 'like'), ('like', 'cake.')]})\n\nromeo_toks = [t.lower() for t in ROMEO_SOLILOQUY.split()]\n\ndct = compute_ngrams(romeo_toks, n=4)\ntc.assertEqual(dct['but'], [('sick', 'and', 'green'), ('fools', 'do', 'wear')])\ntc.assertEqual(dct['it'], \n              [('is', 'the', 'east,'),\n               ('off.', 'it', 'is'),\n               ('is', 'my', 'lady,'),\n               ('is', 'my', 'love!'),\n               ('were', 'not', 'night.')])",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "state": "normal"
      },
      "cell_type": "markdown",
      "source": "I've also placed the entire text of Peter Pan (courtesy of [Project Gutenberg][]) on the server, to be used to stress test your function just a bit. Evaluate the following cell to read the text of the book into `peter_pan_text`.\n\nIf you're not on the course server, you can uncomment the line to read the text directly from the Project Gutenberg website and comment out the lines which access the file for testing.\n\n[Project Gutenberg]: http://gutenberg.org"
    },
    {
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "load_peter_pan",
        "state": "read_only",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import urllib.request\nPETER_PAN_URL      = 'https://moss.cs.iit.edu/cs331/data/peterpan.txt'\n\npeter_pan_text = urllib.request.urlopen(PETER_PAN_URL).read().decode()\nchapt1_start = peter_pan_text.index('All children')\nprint(peter_pan_text[chapt1_start:chapt1_start+1000])",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": "All children, except one, grow up. They soon know that they will grow\r\nup, and the way Wendy knew was this. One day when she was two years old\r\nshe was playing in a garden, and she plucked another flower and ran with\r\nit to her mother. I suppose she must have looked rather delightful, for\r\nMrs. Darling put her hand to her heart and cried, “Oh, why can't you\r\nremain like this for ever!” This was all that passed between them on\r\nthe subject, but henceforth Wendy knew that she must grow up. You always\r\nknow after you are two. Two is the beginning of the end.\r\n\r\nOf course they lived at 14 [their house number on their street], and\r\nuntil Wendy came her mother was the chief one. She was a lovely lady,\r\nwith a romantic mind and such a sweet mocking mouth. Her romantic\r\nmind was like the tiny boxes, one within the other, that come from the\r\npuzzling East, however many you discover there is always one more; and\r\nher sweet mocking mouth had one kiss on it that Wendy could never get,\r\nthough ther\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "state": "normal"
      },
      "cell_type": "markdown",
      "source": "Time for some larger test cases!"
    },
    {
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "test_compute_ngrams_2",
        "nbgrader": {
          "grade": true,
          "grade_id": "test_compute_ngrams_2",
          "locked": true,
          "points": 5,
          "schema_version": 1,
          "solution": false
        },
        "state": "read_only",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# (5 points)\n\nfrom unittest import TestCase\ntc = TestCase()\n\npp_toks = [t.lower() for t in peter_pan_text.split()]\ndct = compute_ngrams(pp_toks, n=3)\ntc.assertEqual(dct['crocodile'], \n               [('passes,', 'but'),\n                ('that', 'happened'),\n                ('would', 'have'),\n                ('was', 'in'),\n                ('passed', 'him,'),\n                ('is', 'about'),\n                ('climbing', 'it.'),\n                ('that', 'was'),\n                ('pass', 'by'),\n                ('and', 'let'),\n                ('was', 'among'),\n                ('was', 'waiting')])\ntc.assertEqual(len(dct['wendy']), 202)\ntc.assertEqual(len(dct['peter']), 243)",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "state": "normal"
      },
      "cell_type": "markdown",
      "source": "### Random selection"
    },
    {
      "metadata": {
        "state": "normal"
      },
      "cell_type": "markdown",
      "source": "One more thing before you start work on generating passages from an n-gram dictionary: we need a way to choose a random item from a sequence.\n\nThe [`random.choice` function](https://docs.python.org/3/library/random.html#random.choice) provides just this functionality. Consider (and feel free to play with) the following examples --- you should, at the very least, evaluate the cell a few separate times to see the results:"
    },
    {
      "metadata": {
        "state": "normal",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import random\nprint(random.choice(['lions', 'tigers', 'bears']))\nprint(random.choice(range(100)))\nprint(random.choice([('really', 'like'), ('like', 'cake')]))",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": "tigers\n79\n('like', 'cake')\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "state": "normal"
      },
      "cell_type": "markdown",
      "source": "Note that a separate tutorial on random number generators (and other [`random` module](https://docs.python.org/3/library/random.html) APIs) will be posted separately, but for now just understanding how to use `random.choice` should be sufficient for this assignment."
    },
    {
      "metadata": {
        "state": "normal"
      },
      "cell_type": "markdown",
      "source": "### `gen_passage`\n\nFinally, you're ready to implement `gen_passage`, which will take an n-gram dictionary and a length for the passage to generate (as a token count). \n\nAs described earlier, it will work as follows:\n\n1. Select a random key from the dictionary and use it as the start token of the passage. It will also serve as the current token for the next step.\n2. Select a random tuple from the list associated with the current token and append the sequence to the passage. The last token of the selected sequence will be the new current token.\n3. If the current token is a key in the dictionary then simply repeat step 2, otherwise select another random key from the map as the current token and append it to the passage before repeating step 2.\n\nYou will use `random.choice` whenever a random selection needs to be made. In order for your results to be reproduceable, be sure to sort the dictionary's keys (which, recall, are in no discernible order) before selecting a random one, like this (assuming `ngram_dict` is the dictionary):\n\n    random.choice(sorted(ngram_dict.keys()))"
    },
    {
      "metadata": {
        "deletable": false,
        "id": "gen_passage",
        "nbgrader": {
          "grade": true,
          "grade_id": "gen_passage",
          "locked": false,
          "points": 5,
          "schema_version": 1,
          "solution": true
        },
        "starter_code": "import random\n\ndef gen_passage(ngram_dict, length=100):\n    ",
        "state": "graded",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import random\n\ndef gen_passage(ngram_dict, length=100):\n    key = random.choice(sorted(ngram_dict.keys()))\n    passage = []\n    passage.append(key)\n    run = True\n    while run:\n        after = random.choice(ngram_dict[key])\n        for i in after:\n            if len(passage) >= length: # to make sure it doesnt go over the word count\n                run = False\n                break\n            else:\n                passage.append(i)\n        key = passage[-1]\n        if key not in ngram_dict:\n            if run: # doesnt add anymore to the passage if reached word count\n                key = random.choice(sorted(ngram_dict.keys()))\n                passage.append(key)\n    out = ' '.join(passage)\n    return(out)",
      "execution_count": 82,
      "outputs": []
    },
    {
      "metadata": {
        "state": "normal"
      },
      "cell_type": "markdown",
      "source": "For the following test cases to work, it is *critical* that you do not invoke `random.choice` more than is absolutely necessary, and only as prescribed in the steps described above!\n\nNote that in addition to the automated test cases, we'll also be manually grading your code above."
    },
    {
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "gen_passage_test",
        "nbgrader": {
          "grade": true,
          "grade_id": "gen_passage_test",
          "locked": true,
          "points": 5,
          "schema_version": 1,
          "solution": false
        },
        "state": "read_only",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# (10 points)\n\nfrom unittest import TestCase\ntc = TestCase()\n\nrandom.seed(1234)\nsimple_toks = [t.lower() for t in 'I really really like cake.'.split()]\ntc.assertEqual(gen_passage(compute_ngrams(simple_toks), 10),\n               'like cake. i really really really really like cake. i')\n\nrandom.seed(1234)\nromeo_toks = [t.lower() for t in ROMEO_SOLILOQUY.split()]\ntc.assertEqual(gen_passage(compute_ngrams(romeo_toks), 10),\n               'too bold, \\'tis not night. see, how she leans her')",
      "execution_count": 83,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.3",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "mimir": {
      "project_id": "279d03b8-3bf6-4fd0-9153-39bc78998a6e",
      "last_submission_id": "",
      "data": {}
    },
    "varInspector": {
      "window_display": false,
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "library": "var_list.py",
          "delete_cmd_prefix": "del ",
          "delete_cmd_postfix": "",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "library": "var_list.r",
          "delete_cmd_prefix": "rm(",
          "delete_cmd_postfix": ") ",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}